# Promptfoo Configuration for Kilig v2.1 (Robust Edition)
# Evaluates agent outputs against high-rigor qualitative and quantitative assertions.

outputPath: tests/results/latest.json

prompts:
  - "{{content}}"

providers:
  - id: file://scripts/pass_through_provider.js
    label: "Artifact Pass-through"

defaultTest:
  options:
    rubricProvider: google:gemini-2.0-flash

# =============================================================================
# ROBUST AGENT EVALUATIONS
# =============================================================================
tests:
  # ---------------------------------------------------------------------------
  # 1. SCIENTIST AGENT: High-Fidelity Critical Analysis
  # ---------------------------------------------------------------------------
  - description: "Scientist Agent - Scientific Rigor & Factuality"
    vars:
      content: file://tests/artifacts/latest/scientist_analysis.txt
    assert:
      # 1.1 Structural Integrity
      - type: javascript
        value: |
          const required = ['Core Concept', 'Methodology', 'Results', 'Validity'];
          const missing = required.filter(s => !output.includes(s));
          return missing.length === 0 ? true : `Missing: ${missing.join(', ')}`;
        weight: 1

      # 1.2 Quantitative Density (Scientific Proof)
      - type: javascript
        value: |
          const numbers = (output.match(/\d+/g) || []).length;
          return numbers >= 5 ? true : `Too few data points (${numbers}/5). Scientific analysis needs metrics.`;
        weight: 1

      # 1.3 Qualitative Depth (LLM Rubric)
      - type: llm-rubric
        value: |
          Critically evaluate this scientific analysis:
          - FIDELITY: Does it explain the 'How' (Methodology) as well as the 'What' (Core Concept)?
          - OBJECTIVITY: Is the tone academic and neutral (avoid words like 'amazing', 'revolutionary', 'game-changing')?
          - INSIGHT: Does the 'Weaknesses' section identify actual limitations (e.g., compute, data bias, edge cases)?
          Result: Pass if it feels like a graduate-level peer review.
        weight: 3

  # ---------------------------------------------------------------------------
  # 2. NARRATIVE AGENT: Logical Storytelling & Renderability
  # ---------------------------------------------------------------------------
  - description: "Narrative Agent - Logical Arc & Manifest Validity"
    vars:
      content: file://tests/artifacts/latest/comic_manifest.json
    assert:
      - type: is-json
        weight: 3

      # 2.1 Narrative Sequencing Logic
      - type: llm-rubric
        value: |
          Analyze the panel sequence in this JSON manifest:
          - FLOW: Does Panal 1 set the stage? Does the middle focus on tech? Does it end with impact/future?
          - VISUAL DIVERSITY: Is every panel description the same, or does it vary camera angles (Close-up vs Wide) and visual types?
          - CLARITY: Could a human illustrator (or DALL-E) draw these panels based ONLY on the descriptions provided?
          Result: Pass if the story flows logically and visuals are well-defined.
        weight: 2

      # 2.2 Visualization Spec Adherence (5-Panel Rule)
      - type: javascript
        value: |
          const manifest = JSON.parse(output);
          const panels = (manifest.pages || []).flatMap(p => p.panels || []);
          if (panels.length < 5) return `Failed 5-panel rule (Only ${panels.length} found)`;
          
          const hasDataViz = panels.some(p => ['chart', 'flow', 'comparison', 'graph'].some(t => (p.type || p.visual_type || '').toLowerCase().includes(t)));
          if (!hasDataViz) return "Narrative missed the requirement to include data visualizations.";
          
          return true;
        weight: 1

  # ---------------------------------------------------------------------------
  # 3. VALIDATOR AGENT: Critical Objectivity & Actionability
  # ---------------------------------------------------------------------------
  - description: "Validator Agent - Critical QC Guardrail"
    vars:
      content: file://tests/artifacts/latest/validator_report.txt
    assert:
      # 3.1 Decision Clarity
      - type: javascript
        value: |
          const statusMatch = output.match(/(PASS|FAIL|REJECTED|APPROVED|VALID|INVALID)/i);
          return !!statusMatch ? true : "QC Report must explicitly state PASS or FAIL status.";
        weight: 2

      # 3.2 Constructive Feedback
      - type: llm-rubric
        value: |
          Evaluate the Validator's feedback:
          - CRITICALITY: Is the validator being too nice? It should be nit-picky about methodology and formatting.
          - SPECIFICITY: If it finds an issue, does it point to exactly *what* is wrong?
          - ACTIONABILITY: Does it give a clear instruction on how the Narrative agent can fix the error?
          Result: Pass if the validator is professional, critical, and specific.
        weight: 2

  # ---------------------------------------------------------------------------
  # 4. RAG PIPELINE: Context Retrieval Precision
  # ---------------------------------------------------------------------------
  - description: "Agentic RAG - Retrieval Integrity"
    vars:
      content: file://tests/artifacts/latest/search_results.json
    assert:
      - type: is-json
        weight: 1
      
      - type: javascript
        value: |
          if (!output || output === '{}') return true;
          const data = JSON.parse(output);
          const hits = data.hits || [];
          const highScores = hits.filter(h => h.score > 0.6);
          return highScores.length > 0 ? true : "Retrieval returned only low-relevance results.";
        weight: 1
